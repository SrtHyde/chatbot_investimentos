{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atualizar\n",
    "- Melhorar prompt \n",
    "- Verificar Persistência dos dados \n",
    "- Testar diferentes modelos\n",
    "- Alterar o parser de pdf para o parser do llama_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import chromadb\n",
    "from glob import glob\n",
    "from PyPDF2 import PdfReader\n",
    "from os import path, getenv\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from llama_index.core import VectorStoreIndex, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classe Config\n",
    "- Define as variáveis de ambiente necessárias para a execução do programa de um arquivo .env.\n",
    "- Iniciliza a conexão com o ChromaDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81775f4c414f46dcb6f2642158bfa211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9078c173686440bca337feb6c7a7bdc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        \n",
    "        load_dotenv()\n",
    "        self.llm_model = getenv('llm_model')\n",
    "        self.embedding_model = getenv('embedding_model')\n",
    "        self.api_key = getenv('api_key')\n",
    "        self.db_path = getenv('db_path')\n",
    "        self.input_dir = getenv('input_dir')\n",
    "        self.collection_name = getenv('collection_name')\n",
    "        self.connection = None\n",
    "\n",
    "    def connect_db(self):\n",
    "        try:\n",
    "            db_client = chromadb.PersistentClient(path=self.db_path)\n",
    "            collection = db_client.get_or_create_collection(self.collection_name)\n",
    "            self.connection = ChromaVectorStore(chroma_collection=collection)\n",
    "            \n",
    "            print(\"Conexão ao ChromaDB estabelecida com sucesso!\")\n",
    "            return self.connection\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Erro ao conectar ao banco de dados: {str(e)}\")\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classe GenerateIndex\n",
    "- Faz o pré processamento dos pdfs dividindo o texto em chunks de 100 palavras\n",
    "- Gera os embbedings e armazena em um banco de dados chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenarateIndex:\n",
    "    def __init__(self):\n",
    "        self.config = Config()\n",
    "        self.vector_store = self.config.connect_db()\n",
    "        self.llm_model = self.config.llm_model\n",
    "        self.embed_model_name = self.config.embedding_model\n",
    "        self.input_dir = self.config.input_dir\n",
    "        self.api_key = self.config.api_key\n",
    "        self.nodes = None\n",
    "\n",
    "    def generate_index(self):\n",
    "        try:\n",
    "            self.embedding_model = HuggingFaceEmbedding(model_name=self.embed_model_name, embed_batch_size = 20)\n",
    "            self.index = VectorStoreIndex(\n",
    "                nodes = self.nodes,\n",
    "                vector_store=self.vector_store,\n",
    "                embed_model=self.embedding_model,\n",
    "                show_progress=True,\n",
    "\n",
    "            )\n",
    "            print(\"Índice gerado com sucesso!\")\n",
    "            return self.index\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Erro ao gerar índice: {str(e)}\")\n",
    "\n",
    "    def split_into_chunks(self, text, chunk_size=100, overlap=10):\n",
    "        \"\"\"\n",
    "        Divide o texto em chunks de palavras com overlap especificado.\n",
    "\n",
    "        :param text: Texto completo que será dividido.\n",
    "        :param chunk_size: Número de palavras em cada chunk.\n",
    "        :param overlap: Quantidade de palavras que se sobrepõem entre chunks.\n",
    "        :return: Lista de chunks de texto.\n",
    "        \"\"\"\n",
    "        words = text.split()  # Divide o texto em uma lista de palavras\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        while start < len(words):\n",
    "            end = start + chunk_size\n",
    "            chunk = \" \".join(words[start:end])  # Junta as palavras para formar o chunk\n",
    "            chunks.append(chunk)\n",
    "            start += chunk_size - overlap  # Avança, considerando o overlap\n",
    "        return chunks\n",
    "    \n",
    "    def process_pdfs(self, input_dir):\n",
    "        files = glob(f\"{input_dir}/*.pdf\")\n",
    "        text_nodes = []\n",
    "        try:\n",
    "            for file in files:\n",
    "                reader = PdfReader(file)\n",
    "                text = \"\"\n",
    "                \n",
    "                for page in reader.pages:\n",
    "                    extracted_text = page.extract_text().replace('\\n', ' ').replace('  ', ' ')\n",
    "                    print\n",
    "                    if extracted_text:\n",
    "                        text += extracted_text + \" \"\n",
    "                \n",
    "                if not text.strip():\n",
    "                    raise ValueError(f\"O PDF {file} parece não conter texto processável.\")\n",
    "                \n",
    "                chunks = self.split_into_chunks(text)\n",
    "                \n",
    "                # Armazenando os chunks de texto e metadados\n",
    "                for chunk in chunks:\n",
    "                    cont =+ 1\n",
    "                    text_nodes.append(TextNode(\n",
    "                    text=chunk,\n",
    "                    metadata={\"filename\": file, \"num_chunk\": cont},\n",
    "                    \n",
    "                ))\n",
    "                # Armazenando o texto extraído e metadados\n",
    "                \n",
    "                print(f\"Texto extraído do arquivo {file} com sucesso!\")\n",
    "            self.nodes = list(text_nodes)\n",
    "            return self.nodes\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Erro ao transformar texto: {str(e)}\")\n",
    "        \n",
    "    def execute(self):\n",
    "        try:\n",
    "            self.process_pdfs(self.input_dir)\n",
    "            self.generate_index()\n",
    "            return self.index\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Erro ao executar o processo: {str(e)}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classe Chatbot\n",
    "- Contém as configurações do modelo de llm\n",
    "- Realiza a busca por similaridade usando o método as_query_engine() do llama_index\n",
    "- Envia a pergunta + a resposta da query para o llm gerar a resposta contextualizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatBot:\n",
    "    def __init__(self, index):\n",
    "        self.config = Config()\n",
    "        self.api_key = self.config.api_key\n",
    "        self.llm_model = self.config.llm_model\n",
    "        self.index = index\n",
    "        \n",
    "\n",
    "    def initialize_llm(self):\n",
    "        try:\n",
    "            self.prompt = (\n",
    "                    \"Essas são suas diretrizes para interagir com os usuários: \"\n",
    "                    \"Seu nome é BOOT. \"\n",
    "                    \"Responda com base no que sabe e adapte suas respostas de forma a se encaixar perfeitamente na pergunta ou necessidade do usuário. \"\n",
    "                    \"Não use os termos 'Resposta:' ou 'Pergunta:' ao iniciar suas mensagens. \"\n",
    "                    \"Você é um assistente virtual projetado para fornecer informações sobre diversos tópicos. \"\n",
    "                    \"Responda exclusivamente em português, garantindo clareza e precisão. \"\n",
    "                    \"Ofereça respostas educadas, amigáveis e sempre no mesmo tom e idioma utilizado pelo usuário. \"\n",
    "                    \"Caso o usuário inicie a conversa com uma saudação ou pergunte quem você é, apresente-se de forma cordial e simpática. \"\n",
    "                    \"Seja educado em todas as interações, objetivo e direto \"\n",
    "                    \"Sempre atenda aos pedidos do usuário em português.\"\n",
    "                    )\n",
    "            Settings.llm = HuggingFaceInferenceAPI(model_name=self.llm_model, \n",
    "                                                   token=self.api_key, \n",
    "                                                   system_prompt=self.prompt,\n",
    "                                                   generate_kwargs={\"temperature\": 0.7})\n",
    "\n",
    "            chat_engine = self.index.as_chat_engine(\n",
    "                chat_mode=\"condense_question\",\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "            return chat_engine\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Erro ao inicializar Modelo: {str(e)}\")\n",
    "        \n",
    "    def search(self, query):\n",
    "        try:\n",
    "            query_engine = self.index.as_query_engine(\n",
    "                llm=HuggingFaceInferenceAPI(model_name=self.llm_model, token=self.api_key),\n",
    "                response_mode=\"tree_summarize\",\n",
    "                max_results=5,\n",
    "                verbose=False\n",
    "            )\n",
    "            response = query_engine.query(query)\n",
    "            context = \" || \".join([doc.node.text for doc in response.source_nodes])\n",
    "            return response, context\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Erro ao realizar a busca: {str(e)}\")\n",
    "        \n",
    "        \n",
    "    def respond(self, message, history=None):\n",
    "        \"\"\"Gera uma resposta baseada no contexto da pesquisa.\"\"\"\n",
    "        try:\n",
    "            \n",
    "            # Busca o contexto no ChromaDB\n",
    "            chat_engine = self.initialize_llm()\n",
    "            response, context = self.search(message)\n",
    "\n",
    "            message_context = f\"Contexto: {context} || Pergunta: {message}\"\n",
    "\n",
    "            # Gera a resposta usando o motor de chat\n",
    "            response = chat_engine.chat(message_context)\n",
    "\n",
    "            return response.response\n",
    "        \n",
    "        except Exception as e:\n",
    "            return f\"Erro ao processar mensagem: {str(e)}\"\n",
    "\n",
    "    def interface(self): \n",
    "        demo = gr.ChatInterface(\n",
    "            fn=self.respond,\n",
    "            type=\"messages\", \n",
    "            examples=[\"hello\", \"hola\", \"merhaba\", \"oi\"], \n",
    "            title=\"Boot\",\n",
    "        )\n",
    "        return demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se conecta ao db e armazena o index (embbeding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_index = GenarateIndex()\n",
    "index = generate_index.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utliza o index para realizar as interações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = ChatBot(index)\n",
    "chatbot.interface()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
