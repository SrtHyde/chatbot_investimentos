{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atualizar\n",
    "- Adicionar persitencia de dados carregados, ou seja, implementar sistema de armazenamento dos vetores ex: chromadb\n",
    "- organizar prompt \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.mistralai import MistralAI\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr \n",
    "from os import path,getenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = getenv(\"api_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 26/26 [00:00<00:00, 60.18it/s]\n",
      "Generating embeddings: 100%|██████████| 87/87 [01:02<00:00,  1.39it/s]\n"
     ]
    }
   ],
   "source": [
    "class ConfigChatIndex:\n",
    "    def __init__(self, input_dir=\"data\"):\n",
    "\n",
    "        if not path.exists(input_dir):\n",
    "            raise ValueError(f\"O diretório {input_dir} não existe\")\n",
    "\n",
    "        self.input_dir = input_dir\n",
    "        self.service_context = None\n",
    "        self.index = None\n",
    "\n",
    "    def initialize_settings(self):\n",
    "        \"\"\"Initialize settings for embedding, and node parsing.\"\"\"\n",
    "        Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "        Settings.node_parser = SentenceSplitter(chunk_size=500, chunk_overlap=50)\n",
    "        Settings.num_output = 512\n",
    "        Settings.context_window = 3900\n",
    "\n",
    "    def build_index(self):\n",
    "        \"\"\"Build the VectorStoreIndex from the input directory.\"\"\"\n",
    "        data = SimpleDirectoryReader(input_dir=self.input_dir, recursive=True).load_data()\n",
    "        self.index = VectorStoreIndex.from_documents(\n",
    "            data,\n",
    "            show_progress=True\n",
    "        )\n",
    "    def execute(self):\n",
    "        try:\n",
    "            self.initialize_settings()\n",
    "            self.build_index()\n",
    "\n",
    "            return self.index\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Erro ao inicializar o chatbot: {str(e)}\")\n",
    "\n",
    "config_index = ConfigChatIndex( input_dir=\"data\")  \n",
    "data = config_index.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chatbot:\n",
    "    def __init__(self, api_key, prompt, data ):\n",
    "\n",
    "        self.api_key = api_key\n",
    "        self.prompt = prompt\n",
    "        self.service_context = None\n",
    "        self.index = data\n",
    "        self.chat_engine = None\n",
    "        self.model_name = \"HuggingFaceH4/zephyr-7b-alpha\"\n",
    "\n",
    "        try:\n",
    "            self.initialize_llm()\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Erro ao inicializar o chatbot: {str(e)}\")\n",
    "        \n",
    "    def initialize_llm(self):\n",
    "        \"\"\"Initialize settings for LLM, embedding, and node parsing.\"\"\"\n",
    "        Settings.llm = MistralAI(api_key=self.api_key, system_prompt=self.prompt, temperature=0.9)\n",
    "        #Settings.llm = HuggingFaceLLM(model_name=self.model_name)\n",
    "        self.chat_engine = self.index.as_chat_engine(\n",
    "            chat_mode=\"condense_question\",\n",
    "            verbose=True)\n",
    "    \n",
    "        \n",
    "    def respond(self, message, history = None):\n",
    "        try:\n",
    "            if not self.chat_engine:\n",
    "                raise ValueError(\"Chat engine is not initialized.\")\n",
    "            response = self.chat_engine.chat(message)\n",
    "            return response.response\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar mensagem: {str(e)}\")\n",
    "            return \"Desculpe, ocorreu um erro ao processar sua solicitação.\"\n",
    "\n",
    "prompt = (\n",
    "    \"Responda apenas em Português\"\n",
    "    \"Quando te derem alguma saudação, ou perguntgar sobre quem é, você deve se apresentar\"\n",
    "    \"Você é um assistente que responde a perguntas sobre relatórios de investimentos de empresas\"\n",
    "    \"Responda de forma clara e direta às perguntas feitas, com base nos documentos, indicando o tempo, periodo ou data de cada resultado.\"\n",
    "    )\n",
    "\n",
    "load_dotenv()\n",
    "chatbot_instance = Chatbot(api_key=api_key, prompt=prompt, data=data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chatbot_interface(message,historico=None):\n",
    "    try:\n",
    "        response = chatbot_instance.respond(message, historico)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"Desculpe, ocorreu um erro ao processar sua solicitação: {str(e)}\"\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chatbot_interface,\n",
    "    type=\"messages\", \n",
    "    examples=[\"hello\", \"hola\", \"merhaba\", \"oi\"], \n",
    "    title=\"Chatbot de Investimentos\",\n",
    ")\n",
    "\n",
    "\n",
    "#demo = gr.Interface(fn=chatbot_interface, \n",
    "#                    inputs=\"text\", \n",
    "#                    outputs=\"text\", \n",
    "#                    title=\"Chatbot de Investimentos\", \n",
    "#                    description=\"Chatbot de Investimentos para responder perguntas sobre relatórios de investimentos de empresas\")\n",
    "#\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
